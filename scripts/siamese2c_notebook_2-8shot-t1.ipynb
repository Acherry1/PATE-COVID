{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import utils as utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a16c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teachers=2\n",
    "teacher_id = 1\n",
    "epochs = 175\n",
    "shot = 8\n",
    "SIAMESE_MODEL_NAME = '..\\scripts\\models_h5\\siamese_network2c-t{}_{}notebook.h5'.format(teacher_id,shot)\n",
    "EMBEDDING_MODEL_NAME = 'embedding_network2w_0801.h5'\n",
    "if os.path.exists(SIAMESE_MODEL_NAME):\n",
    "    os.remove(SIAMESE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b86c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 2 classes.\n",
      "The train set contains 16\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"..\\scripts\\dataset\"\n",
    "dataset_path = os.path.join( r\"..\\scripts\\auto_datasets\", r\"auto_t{}_{}\\t{}-{}\".format(all_teachers,shot,teacher_id, shot))\n",
    "result_file_path = os.path.join(base_dir, r\"true_result_xlsx\")\n",
    "pre_results_path = os.path.join(base_dir, r\"teacher_results_xlsx\",\"result_{}_{}\".format(all_teachers,shot))\n",
    "train_image_list, train_y_list = utils.load_images(dataset_path, 'train', (100, 100))\n",
    "print(\"The train set contains\", len(train_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8942f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The valid set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_path = r\"../scripts/dataset/pretrain_2c_0727/\"\n",
    "valid_image_list, valid_y_list = utils.load_images(test_path, 'valid', (100, 100))\n",
    "print(\"The valid set contains\", len(valid_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ff552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The test set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_image_list, test_y_list = utils.load_images(test_path, 'test', (100, 100))\n",
    "print(\"The test set contains\", len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369794ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "num_classes: 2\n",
      "num_classes: 2\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train, source_labels_train, true_labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val, source_labels_val, true_labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_test, labels_test, source_labels_test, true_labels_test = utils.make_pairs(test_image_list, test_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8959873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c8f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for train 32\n",
      "number of pairs for validation 2400\n",
      "number of pairs for test 2400\n"
     ]
    }
   ],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x1(如何给标签带上)\n",
    "x_train_2 = pairs_train[:, 1]  # x2\n",
    "print(\"number of pairs for train\", np.shape(x_train_1)[0])\n",
    "\n",
    "x_val_1 = pairs_val[:, 0]\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0])\n",
    "\n",
    "x_test_1 = pairs_test[:, 0]\n",
    "x_test_2 = pairs_test[:, 1]\n",
    "# print(x_test_1)\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6955c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(result_file_path):\n",
    "    os.makedirs(result_file_path)\n",
    "result_xlsx=os.path.join(result_file_path,\"true_labels.xlsx\")\n",
    "if not os.path.exists(result_xlsx):\n",
    "    test_1_image_list = x_test_1.tolist()\n",
    "    df = pd.DataFrame({\"image\": test_1_image_list, \"true_label\": true_labels_test})\n",
    "    df.to_excel(result_xlsx, index=False)\n",
    "    df = pd.read_excel(result_xlsx)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_excel(result_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80363b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_1 = Input((100, 100, 3))\n",
    "input_2 = Input((100, 100, 3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_NAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for layer in embedding_network.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2'))\n",
    "\n",
    "output_1 = model(input_1)\n",
    "output_2 = model(input_2)\n",
    "\n",
    "merge_layer = Lambda(utils.manhattan_distance)([output_1, output_2])\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=SIAMESE_MODEL_NAME, verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b049d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.5625\n",
      "Epoch 00001: val_loss improved from inf to 0.33306, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 205s 7s/step - loss: 0.2127 - accuracy: 0.5625 - val_loss: 0.3331 - val_accuracy: 0.5088 - lr: 1.0000e-04\n",
      "Epoch 2/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.2004 - accuracy: 0.5625\n",
      "Epoch 00002: val_loss improved from 0.33306 to 0.31920, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 205s 7s/step - loss: 0.2004 - accuracy: 0.5625 - val_loss: 0.3192 - val_accuracy: 0.5163 - lr: 1.0000e-04\n",
      "Epoch 3/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.6250\n",
      "Epoch 00003: val_loss improved from 0.31920 to 0.30263, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 211s 7s/step - loss: 0.1889 - accuracy: 0.6250 - val_loss: 0.3026 - val_accuracy: 0.5263 - lr: 1.0000e-04\n",
      "Epoch 4/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1776 - accuracy: 0.6562\n",
      "Epoch 00004: val_loss improved from 0.30263 to 0.28846, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 207s 7s/step - loss: 0.1776 - accuracy: 0.6562 - val_loss: 0.2885 - val_accuracy: 0.5379 - lr: 1.0000e-04\n",
      "Epoch 5/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.7188\n",
      "Epoch 00005: val_loss improved from 0.28846 to 0.27326, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 204s 7s/step - loss: 0.1683 - accuracy: 0.7188 - val_loss: 0.2733 - val_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 6/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.7500\n",
      "Epoch 00006: val_loss improved from 0.27326 to 0.26126, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 201s 6s/step - loss: 0.1596 - accuracy: 0.7500 - val_loss: 0.2613 - val_accuracy: 0.5608 - lr: 1.0000e-04\n",
      "Epoch 7/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.7500\n",
      "Epoch 00007: val_loss improved from 0.26126 to 0.24614, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 204s 7s/step - loss: 0.1529 - accuracy: 0.7500 - val_loss: 0.2461 - val_accuracy: 0.5788 - lr: 1.0000e-04\n",
      "Epoch 8/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1471 - accuracy: 0.7500\n",
      "Epoch 00008: val_loss improved from 0.24614 to 0.23320, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 202s 7s/step - loss: 0.1471 - accuracy: 0.7500 - val_loss: 0.2332 - val_accuracy: 0.5975 - lr: 1.0000e-04\n",
      "Epoch 9/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.7500\n",
      "Epoch 00009: val_loss improved from 0.23320 to 0.21873, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 202s 6s/step - loss: 0.1429 - accuracy: 0.7500 - val_loss: 0.2187 - val_accuracy: 0.6250 - lr: 1.0000e-04\n",
      "Epoch 10/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.7500\n",
      "Epoch 00010: val_loss improved from 0.21873 to 0.21108, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 201s 6s/step - loss: 0.1391 - accuracy: 0.7500 - val_loss: 0.2111 - val_accuracy: 0.6433 - lr: 1.0000e-04\n",
      "Epoch 11/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.7500\n",
      "Epoch 00011: val_loss improved from 0.21108 to 0.20174, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 199s 6s/step - loss: 0.1381 - accuracy: 0.7500 - val_loss: 0.2017 - val_accuracy: 0.6617 - lr: 1.0000e-04\n",
      "Epoch 12/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.8125\n",
      "Epoch 00012: val_loss did not improve from 0.20174\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1364 - accuracy: 0.8125 - val_loss: 0.2116 - val_accuracy: 0.6513 - lr: 1.0000e-04\n",
      "Epoch 13/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.8125\n",
      "Epoch 00013: val_loss did not improve from 0.20174\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1364 - accuracy: 0.8125 - val_loss: 0.2069 - val_accuracy: 0.6604 - lr: 1.0000e-04\n",
      "Epoch 14/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.8125\n",
      "Epoch 00014: val_loss did not improve from 0.20174\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1356 - accuracy: 0.8125 - val_loss: 0.2050 - val_accuracy: 0.6667 - lr: 1.0000e-04\n",
      "Epoch 15/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.8125\n",
      "Epoch 00015: val_loss did not improve from 0.20174\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1346 - accuracy: 0.8125 - val_loss: 0.2091 - val_accuracy: 0.6637 - lr: 1.0000e-04\n",
      "Epoch 16/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1339 - accuracy: 0.8125\n",
      "Epoch 00016: val_loss improved from 0.20174 to 0.19846, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 198s 6s/step - loss: 0.1339 - accuracy: 0.8125 - val_loss: 0.1985 - val_accuracy: 0.6829 - lr: 1.0000e-04\n",
      "Epoch 17/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1329 - accuracy: 0.8125\n",
      "Epoch 00017: val_loss did not improve from 0.19846\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1329 - accuracy: 0.8125 - val_loss: 0.2058 - val_accuracy: 0.6762 - lr: 1.0000e-04\n",
      "Epoch 18/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.8125\n",
      "Epoch 00018: val_loss improved from 0.19846 to 0.19630, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_8notebook.h5\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1331 - accuracy: 0.8125 - val_loss: 0.1963 - val_accuracy: 0.6908 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.8125\n",
      "Epoch 00019: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1317 - accuracy: 0.8125 - val_loss: 0.2048 - val_accuracy: 0.6800 - lr: 1.0000e-04\n",
      "Epoch 20/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.8125\n",
      "Epoch 00020: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1315 - accuracy: 0.8125 - val_loss: 0.1990 - val_accuracy: 0.6904 - lr: 1.0000e-04\n",
      "Epoch 21/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.8438\n",
      "Epoch 00021: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1308 - accuracy: 0.8438 - val_loss: 0.1972 - val_accuracy: 0.6950 - lr: 1.0000e-04\n",
      "Epoch 22/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.8438\n",
      "Epoch 00022: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1302 - accuracy: 0.8438 - val_loss: 0.1998 - val_accuracy: 0.6933 - lr: 1.0000e-04\n",
      "Epoch 23/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.8750\n",
      "Epoch 00023: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1300 - accuracy: 0.8750 - val_loss: 0.1997 - val_accuracy: 0.6950 - lr: 1.0000e-04\n",
      "Epoch 24/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1293 - accuracy: 0.8438\n",
      "Epoch 00024: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 196s 6s/step - loss: 0.1293 - accuracy: 0.8438 - val_loss: 0.1999 - val_accuracy: 0.6950 - lr: 2.0000e-05\n",
      "Epoch 25/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.8750\n",
      "Epoch 00025: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1290 - accuracy: 0.8750 - val_loss: 0.1982 - val_accuracy: 0.6979 - lr: 2.0000e-05\n",
      "Epoch 26/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1290 - accuracy: 0.8750\n",
      "Epoch 00026: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1290 - accuracy: 0.8750 - val_loss: 0.1974 - val_accuracy: 0.6996 - lr: 2.0000e-05\n",
      "Epoch 27/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.8750\n",
      "Epoch 00027: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1288 - accuracy: 0.8750 - val_loss: 0.1977 - val_accuracy: 0.6996 - lr: 2.0000e-05\n",
      "Epoch 28/175\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.8750\n",
      "Epoch 00028: val_loss did not improve from 0.19630\n",
      "32/32 [==============================] - 197s 6s/step - loss: 0.1287 - accuracy: 0.8750 - val_loss: 0.1970 - val_accuracy: 0.7013 - lr: 2.0000e-05\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "                      labels_train,\n",
    "                      validation_data=([x_val_1, x_val_2], labels_val),\n",
    "                      batch_size=1,\n",
    "                      epochs=epochs,  # 175 for contrastive 100 for cross ent\n",
    "                      callbacks=[checkpointer, early_stopping, reduce_lr]\n",
    "                      )\n",
    "# print()\n",
    "# Plot the accuracy\n",
    "# utils.plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "# utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a897dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 163s 2s/step - loss: 0.1453 - accuracy: 0.8792\n",
      "test loss, test acc: [0.14528635144233704, 0.8791666626930237]\n"
     ]
    }
   ],
   "source": [
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5020c970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on validation data\n",
      "Accuracy: 0.8791666666666667\n",
      "Precision: 0.9026845637583892\n",
      "Recall: 0.8791666666666667\n",
      "ROC AUC: 0.8791666666666667\n",
      "F1: 0.8773762751281735\n"
     ]
    }
   ],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "# 返回的是TRUE或FALSE,没有标签数据怎么知道他被分到哪儿个类中？\n",
    "y_pred = Y_pred > 0.5\n",
    "# x1,和x2是否匹配：匹配1，不匹配0\n",
    "y_test = labels_test\n",
    "print(\"\\nEvaluate on validation data\")\n",
    "Accuracy=accuracy_score(y_test, y_pred)\n",
    "Precision=precision_score(y_test, y_pred, average='weighted')\n",
    "Recall=recall_score(y_test, y_pred, average='weighted')\n",
    "ROC_AUC=roc_auc_score(y_test, y_pred, average='weighted')\n",
    "F1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "print(\"Precision:\", Precision)\n",
    "print(\"Recall:\", Recall)\n",
    "print(\"ROC AUC:\", ROC_AUC)\n",
    "print(\"F1:\",F1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if i else 0 for i in y_pred]\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] == 1:\n",
    "        pred_labels += [source_labels_test[i]]\n",
    "    else:\n",
    "        if source_labels_test[i] == 1:\n",
    "            pred_labels += [0.0]\n",
    "        else:\n",
    "            pred_labels += [1.0]\n",
    "a = x_test_1.tolist()\n",
    "df = pd.DataFrame({\"image\": a, \"label_{}\".format(teacher_id): pred_labels})\n",
    "if not os.path.exists(pre_results_path):\n",
    "    os.makedirs(pre_results_path)\n",
    "df.to_excel(os.path.join(pre_results_path,\"{}_{}_{}.xlsx\".format(epochs,teacher_id,shot)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ad007f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEKCAYAAABkEVK1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcaklEQVR4nO3deZhdVZ3u8e9blaoMEEhCBkISSNAwRJTBCEG8GEAhYF+j/TiAXEGbvqgNSKO2gtoXG8WrT9uNoqA3SAS7MWHSTmhppkBucCCE+ZKESAyEJCRkDpiQpOrU7/5xVpEiJFVnV9Wpc+rs9/M8+8nea6+z99pVT/2yhrPWVkRgZpY3dZUugJlZJTj4mVkuOfiZWS45+JlZLjn4mVkuOfiZWS45+JlZxUiaLmmtpGfbpP2zpOckPSPpN5IGtTl3haSlkpZIOqNN+pSUtlTS5aXc28HPzCrpJmDKbmn3A0dFxLuAPwFXAEiaAJwNvCN95npJ9ZLqgeuAM4EJwDkpb7sc/MysYiJiHrBxt7T7IqI5HT4CjE77U4GZEbEjIl4AlgLHp21pRCyLiJ3AzJS3XX266Rm6xdAh9TF2TEOli2EZ/OmZAZUugmWwna3sjB3qyjXOOGWf2LCxUFLex5/ZsRDY3iZpWkRMy3C7vwFuTfujKAbDVitTGsCK3dJP6OjCVRX8xo5p4NF7x1S6GJbBGQcdU+kiWAbzY06Xr7F+Y4H5947uOCPQMPLP2yNiYmfuI+kbQDNwS2c+35GqCn5m1hsEhWgp6x0kfQb4K+C02LUAwSqgbe1odEqjnfS9cp+fmWUSQAtR0tYZkqYAXwU+HBHb2pyaDZwtqa+kccB44FFgATBe0jhJjRQHRWZ3dB/X/Mwssxa6p+YnaQYwGRgqaSVwJcXR3b7A/ZIAHomIz0fEQkm3AYsoNocviohCus7FwL1APTA9IhZ2dG8HPzPLJAiauqnZGxHn7CH5xnbyXw1cvYf0u4G7s9zbwc/MMgmg0MkmbTVx8DOzzDrbn1dNHPzMLJMACjWwAryDn5llVt4vuvQMBz8zyyQI9/mZWf5EQFPvj30OfmaWlSjQpenBVcHBz8wyCaDFNT8zyyPX/Mwsd4pfcnbwM7OcCaApev+aKA5+ZpZJIAo1sCCUg5+ZZdYSbvaaWc64z8/MckoU3OdnZnlTXMnZwc/MciZC7Iz6Shejyxz8zCyzFvf5mVneFAc83Ow1s9zxgIeZ5ZAHPMwstwr+krOZ5U0gmqL3h47e/wRm1qM84GFmuRTIzV4zyycPeJhZ7kRQE1916f1PYGY9qjjgUV/S1hFJ0yWtlfRsm7Qhku6X9Hz6d3BKl6RrJS2V9Iyk49p85vyU/3lJ55fyHA5+ZpZZgbqSthLcBEzZLe1yYE5EjAfmpGOAM4HxabsQ+CkUgyVwJXACcDxwZWvAbI+Dn5llEoiWKG3r8FoR84CNuyVPBW5O+zcDH2mT/ssoegQYJGkkcAZwf0RsjIhNwP28NaC+hfv8zCyzMn/VZURErE77a4ARaX8UsKJNvpUpbW/p7XLwM7NMiu/tLTn4DZX0WJvjaRExreR7RYSksrwl2MHPzDJSlmXs10fExIw3eEXSyIhYnZq1a1P6KmBMm3yjU9oqYPJu6XM7uon7/Mwsk+KrK7tntHcvZgOtI7bnA7PapJ+XRn0nAVtS8/he4HRJg9NAx+kprV2u+ZlZJhHK0uxtl6QZFGttQyWtpDhq+z3gNkkXAMuBT6TsdwNnAUuBbcBni+WJjZK+DSxI+a6KiN0HUd7Cwc/MMuuuLzlHxDl7OXXaHvIGcNFerjMdmJ7l3g5+ZpZJcT0/z+01s9zxSs5mlkPFr7q45mdmOdM6t7e3c/Azs8y8pJWZ5U5xSSs3e80sh9znZ2a5U1zVxc1eM8uZ4vQ2B79c+pfLxjD/gf0YNLSZaQ8tAeCGqw7ikfv3o6ExGHnIDr58zQr23b8AwMwfD+eeGQdQXxd84TurmDj5NQAWPDSQn/3jKAot4sxzNvDJS9bu9Z7WMyZOfpXPf/tl6uuC/5oxhNt+MqLjD+VObdT8yvoEkqZIWpKWnb6840/0Dqd/ciNX37LsTWnHnfwa0x56jp/NWcKoQ3cw88fDAVj+p77MnTWYaQ89x9W/WsZPrhhNoQCFAlz39dF855Zl3DD3OR6aNZjlf+pbicexpK4uuOi7q/jmueP4n5MP55Spmzl4/PZKF6sqtaCStmpWtuAnqR64juLS0xOAcyRNKNf9etI7J21l4ODCm9LePfk16lM9+sh3b2P96gYA/njv/kyeuonGvsGBB+/koLE7WPLkAJY8OYCDxu5g5CE7aWgMJk/dxB/v3b+nH8XaOPzYbbz8YiNrXupLc1Mdc2cN4sQztlS6WFWndbS3lK2albPmdzywNCKWRcROYCbFZahr3r0zhvCeU4tN2/WrGxh2UNMb54aObGLDmgY2rHlremvAtMo44MAm1r3c+Mbx+tUNDB3Z1M4n8qsl6kraqlk5S1fS0tKSLpT0mKTH1m0o7H661/nVj0ZQ3yc49a83VbooZmXRne/wqKSKD3ikJa2nAUw8ul9ZlqvuKffdOoRHH9iP7926FKXf+9CRTax7eVeNbv3qBg44sFib2D3dtYzKKtbGd75x7Nr4ngXQXOW1ulKU8wn2tuR0TVrw0EBuv34437ppGf0G7Irhk05/lbmzBrNzh1jzUiOrXujL4cdu4/BjtrHqhb6seamRpp1i7qzBTDr91Qo+gS15agCjxu1kxJgd9GloYfLUzTxyn/th96QWmr3lrPktAMZLGkcx6J0NfKqM9+sx//sLh/DMH/dly8Y+nPvuCXz6y2uY+ZMRNO0QV3zy7QAc8e6tXPr9lYw9fDsn//fNXDj5COrrg4u/u5L6NCf8oqtX8vVPHUpLQZx+9kbGHu6RxUpqKYjrvjGK7/5qGXX1cN/MISz/U79KF6v69IImbSlUXBy1TBeXzgJ+CNQD0yPi6vbyTzy6Xzx675j2sliVOeOgYypdBMtgfszh1djYpcg1+Ijhcer0j5WU99cn/fTxTrzAqEeUtc8vIu6muO6+mdWQWqj5VXzAw8x6Fy9mama5FIjmluoezCiFg5+ZZVbtU9dK4eBnZtmEm71mlkPu8zOz3HLwM7PcCUTBAx5mlkce8DCz3IkaGfDo/XVXM+txESpp64ikyyQtlPSspBmS+kkaJ2l+WgH+VkmNKW/fdLw0nR/blWdw8DOzjLpnPT9Jo4AvAhMj4iiKawCcDXwfuCYi3g5sAi5IH7kA2JTSr0n5Os3Bz8wy666aH8Wut/6S+gADgNXAqcAd6fzNwEfS/tR0TDp/mqROt78d/MwskwgotKikDRjaulJ72i7cdZ1YBfwAeIli0NsCPA5sjojmlK3tCvBvrA6fzm8BDujsc3jAw8wyyzDau35vS1pJGkyxNjcO2AzcDkzpjvKVwjU/M8sk6LZm7weAFyJiXUQ0Ab8GTgIGpWYwvHkF+DdWh0/n9wc2dPY5HPzMLKNue4HRS8AkSQNS391pwCLgIaB1tdTzgVlpf3Y6Jp1/MLqwGrObvWaWWXcsAB8R8yXdATwBNANPUnyZ2W+BmZK+k9JuTB+5Efg3SUuBjRRHhjvNwc/MMitxJLeE68SVwJW7JS+j+N7v3fNuBz7eLTfGwc/MMiqO9vb+HjMHPzPLrIzvPesxDn5mlll3NXsrycHPzDIJSp69UdUc/Mwssxpo9Tr4mVlGAdHimp+Z5ZCbvWaWSzU92ivpx7TTtI+IL5alRGZW1Vrn9vZ27dX8HuuxUphZ7xFALQe/iLi57bGkARGxrfxFMrNqVwvN3g7nqEg6UdIi4Ll0fLSk68teMjOrUiJaStuqWSkT9H4InEFaNysingZOLmOZzKzaRYlbFStptDciVuy2VH6hPMUxs6oXtT/g0WqFpPcCIakBuBRYXN5imVlVq/JaXSlKafZ+HriI4stDXgaOScdmllsqcateHdb8ImI9cG4PlMXMeouWSheg60oZ7T1U0l2S1klaK2mWpEN7onBmVoVav+dXylbFSmn2/gq4DRgJHETx9XIzylkoM6tuEaVt1ayU4DcgIv4tIprT9u9Av3IXzMyqWC1/1UXSkLT7X5IuB2ZSfJxPAnf3QNnMrFpVeZO2FO0NeDxOMdi1PuXn2pwL4IpyFcrMqpuqvFZXivbm9o7ryYKYWS8RgiqfulaKkmZ4SDoKmECbvr6I+GW5CmVmVa6Wa36tJF0JTKYY/O4GzgR+Bzj4meVVDQS/UkZ7PwacBqyJiM8CRwP7l7VUZlbdanm0t43XI6JFUrOk/YC1wJgyl8vMqlWNLGZaSs3vMUmDgBsojgA/AfyxnIUys+qmKG3r8DrSIEl3SHpO0uK0fugQSfdLej79OzjllaRrJS2V9Iyk47ryDB0Gv4j4u4jYHBE/Az4InJ+av2aWV93X7P0RcE9EHEGxS20xcDkwJyLGA3PSMRTHG8an7ULgp115hPa+5LzXqCrpuIh4ois3NrPeqzu+5ydpf4oLI38GICJ2AjslTaU4yApwMzAX+BowFfhlRATwSKo1joyI1Z25f3t9fv/SzrkATu3MDduzZPlQ3v+5C7v7slZGh81fWOkiWAZPnldKT1cJSu/zGyqp7cvQpkXEtLQ/DlgH/ELS0RS71S4FRrQJaGuAEWl/FLCizbVWprTuDX4RcUpnLmhmNS7bSO76iJi4l3N9gOOASyJivqQfsauJW7xVREjlmU/STf8NmFmudE+f30pgZUTMT8d3UAyGr0gaCZD+XZvOr+LN3zQZndI6xcHPzDJTS2lbeyJiDcXXZByekk4DFgGzgfNT2vnArLQ/GzgvjfpOArZ0tr8PSpzeZmb2Jt3XEL0EuEVSI7AM+CzFStltki4AlgOfSHnvBs4ClgLbUt5OK2V6myguY39oRFwl6WDgwIh4tCs3NrPeqdTv8JUiIp4C9tQneNoe8gbd+P6gUpq91wMnAuek49eA67qrAGbWC9XAMvalNHtPiIjjJD0JEBGbUhXVzPKqyuftlqKU4NckqZ70uJKGURPvbjKzzqrpxUzbuBb4DTBc0tUUV3n5ZllLZWbVKzoeye0NSnlv7y2SHqfYASngIxGxuOwlM7PqlYeaXxrd3Qbc1TYtIl4qZ8HMrIrlIfgBv2XXi4z6UZyPtwR4RxnLZWZVLBd9fhHxzrbHabWXvytbiczMekDmGR4R8YSkE8pRGDPrJfJQ85P0pTaHdRQnHr9cthKZWXXLy2gvMLDNfjPFPsA7y1McM+sVar3ml77cPDAivtJD5TGzKidqfMBDUp+IaJZ0Uk8WyMx6gVoOfsCjFPv3npI0G7gd2Np6MiJ+XeaymVk16sZVXSqplD6/fsAGiu/saP2+XwAOfmZ5VeMDHsPTSO+z7Ap6rWog7ptZZ9V6za8e2Jc3B71WNfDoZtZpNRAB2gt+qyPiqh4riZn1Dtne3la12gt+1b0Mq5lVTK03e9+yhr6ZGVDbNb+I2NiTBTGz3iMv09vMzHbJQZ+fmdlbiNoYEHDwM7PsXPMzszyq9dFeM7M9c/Azs9ypkcVM6ypdADPrhaLErQSS6iU9Kek/0/E4SfMlLZV0q6TGlN43HS9N58d25REc/MwsM0VpW4kuBdq+C/z7wDUR8XZgE3BBSr8A2JTSr0n5Os3Bz8yy66aan6TRwIeAn6djUVw+746U5WbgI2l/ajomnT8t5e8UBz8zyyxDzW+opMfabBfudqkfAl9l1wqBBwCbI6I5Ha8ERqX9UcAKgHR+S8rfKR7wMLNsgiyLma6PiIl7OiHpr4C1EfG4pMndUrYMHPzMLJNufIHRScCHJZ1FccX4/YAfAYNa3yEEjAZWpfyrgDHASkl9gP0prjLfKW72mll23dDnFxFXRMToiBgLnA08GBHnAg8BH0vZzgdmpf3Z6Zh0/sGI6HQYdvAzs8wUUdLWSV8DviRpKcU+vRtT+o3AASn9S8DlXXkGN3vNLJsyrOoSEXOBuWl/GXD8HvJsBz7eXfd08DOzzDy318xyqRamtzn4mVl2rvmZWe5km7pWtRz8zCw7Bz8zy5tu/JJzRTn4mVlmaun90c/Bz8yy8dvbDGDY4L/wjc/OZfDA1wngroeP5M4Hj+Jtozfw5XN/R/++TazZMJBv33gK27Y3AnDulKc466QltLSIa289kQWLxlT2IXLo1ZlNbJ3VBAH7TG1gv3Ma2DanmS037KTpxWDEL/rR98j6N/JvuWknW+9qhjoY/OVG+k/K95+Ov+rSDknTgdZVG44q130qrVCo47rbJ/H8iqH077uTG77xGx5bPIqvfnoe198xiaefH8lZ713C2ac/w/TZEzlk5CZOnfhnPvNPH+OA/bfyr5fdzf/4x0/QEp5p2FN2/rmFrbOaGPGL/qgPrP377fR/Xz0Nh9Yx9Pv92Pi9HW/K37SshW33Fxg5oz+F9cHai7fT7/Z6VF8LL3DspBqo+ZXzL+4mYEoZr18VNr46gOdXDAXg9R2NLF89mGGDtjJ6xBaefv5AABYsHsX7j30BgPcdvZwHH3sbTc31rNmwH6vW7seR49ZVrPx51PxiC43vqKeun1Af0e/Yel6f20zDuDoaDnnrn8S2ec0M+GA9ahR9Dqqjz+g6di6qgapPF3TzSs4VUbbgFxHzgI3lun41OvCA1xh/8HoWvTCcF18ezPuOXg7AKe9exvAhWwEYOmgrazft88Zn1m3ah6GDtlakvHnVcGgdO54qUNgStGwPXv9DgeZX9v6XWlgX9Bmx60+lfrgorK3yv+xyCiCitK2KVbzjIq3seiFA3/6DKluYLujft4mrPvcAP77tRLZtb+T7N7+fL579B8770JP8/pmDaWp2s7ZaNIyrY7/zGlh7yXbq+kPjYXXIv55M3OfXDSJiGjANYOCg0dX9X8Ve1Ne1cNXn7ueBR9/Gw0+OA+ClVwbxlR+dBcDo4Zs58agVAKzfvA/DB++q6Q0bvJX1m/d560WtrPb9cAP7frgBgM3X76R++N777+qHieZXdv21F9ZGu/lrXa18z8//33VZ8LXz/i/L1wzmtgfe9UbqoIGvAyAF5531JLPnHQnA758+mFMn/pmGPgUOPOBVRg9/lcUvDKtIyfOssLH419u8poVtc5vZ54y91wP6n9yHbfcXiJ1B88stNK1ooXFCjv90Sm3yutlb2975tlc448Sl/HnlEH7+zTsBuOE/3sPo4a/y0ckLAZj35Dju/sNhALy4eggPPX4oN3/rdgqFOn444ySP9FbA+su3U9gSqI8Y8g99qRsots1tZtMPdlLYHKy7bDuNh9Uz/Np+NB5ax4AP1LP67NehHob8Q2O+R3qpjZqfurAKdPsXlmYAk4GhwCvAlRFxY3ufGThodBzz/kvLUh4rj8O+ubDSRbAMZp33n6xfvL5LkXvgoNFx7Mml/Z0+fNdXH9/bC4wqrWw1v4g4p1zXNrPKqoWan5u9ZpZNAIXeH/0c/MwsM9f8zCyfqnwktxQOfmaWmWt+ZpY/XtLKzPJIgDzgYWZ5JPf5mVnuuNlrZvlU/fN2S+FJpWaWWXcsZippjKSHJC2StFDSpSl9iKT7JT2f/h2c0iXpWklLJT0j6biuPIODn5ll1z2rujQDX46ICcAk4CJJE4DLgTkRMR6Yk44BzgTGp+1C4KddeQQHPzPLJoqjvaVs7V4mYnVEPJH2XwMWA6OAqcDNKdvNwEfS/lTgl1H0CDBI0sjOPoaDn5llFyVuJZI0FjgWmA+MiIjV6dQaYETaHwWsaPOxlSmtUzzgYWaZZfiqy1BJj7U5npZWb991LWlf4E7g7yPiVWnXilsREVJ55pM4+JlZdqUHv/XtrecnqYFi4LslIn6dkl+RNDIiVqdm7dqUvgpo+5Lr0SmtU9zsNbNsAmgpcWuHilW8G4HFEfGvbU7NBs5P++cDs9qkn5dGfScBW9o0jzNzzc/MMhHRXTM8TgI+Dfw/SU+ltK8D3wNuk3QBsBz4RDp3N3AWsBTYBny2Kzd38DOz7Fq6/u7KiPgdxanCe3LaHvIHcFGXb5w4+JlZNq3N3l7Owc/MMvPCBmaWTw5+ZpY/tbGwgYOfmWXjt7eZWV65z8/M8snBz8xyJ4AWBz8zyx0PeJhZXjn4mVnuBFDo/VM8HPzMLKOAcPAzszxys9fMcsejvWaWW675mVkuOfiZWe5EQKFQ6VJ0mYOfmWXnmp+Z5ZKDn5nlT3i018xyKCD8JWczyyVPbzOz3InolldXVpqDn5ll5wEPM8ujcM3PzPLHi5maWR55YQMzy6MAogamt9VVugBm1stEWsy0lK0DkqZIWiJpqaTLe6D0b3DNz8wyi25o9kqqB64DPgisBBZImh0Ri7p88RK45mdm2XVPze94YGlELIuIncBMYGrZy54oqmjURtI6YHmly1EGQ4H1lS6EZVKrv7NDImJYVy4g6R6KP59S9AO2tzmeFhHT0nU+BkyJiL9Nx58GToiIi7tSvlJVVbO3q7+UaiXpsYiYWOlyWOn8O9u7iJhS6TJ0Bzd7zaxSVgFj2hyPTmk9wsHPzCplATBe0jhJjcDZwOyeunlVNXtr2LRKF8Ay8++szCKiWdLFwL1APTA9Ihb21P2rasDDzKynuNlrZrnk4GdmueTgV0aVnLpjnSNpuqS1kp6tdFmsvBz8yqTN1J0zgQnAOZImVLZUVoKbgJr4Hpu1z8GvfCo6dcc6JyLmARsrXQ4rPwe/8hkFrGhzvDKlmVkVcPAzs1xy8Cufik7dMbP2OfiVT0Wn7phZ+xz8yiQimoHWqTuLgdt6cuqOdY6kGcAfgcMlrZR0QaXLZOXh6W1mlkuu+ZlZLjn4mVkuOfiZWS45+JlZLjn4mVkuOfj1IpIKkp6S9Kyk2yUN6MK1bkpvz0LSz9tbdEHSZEnv7cQ9XpT0lrd87S19tzx/yXivb0n6StYyWn45+PUur0fEMRFxFLAT+Hzbk5I69VqCiPjbDl4UPRnIHPzMqpmDX+/1MPD2VCt7WNJsYJGkekn/LGmBpGckfQ5ART9J6ws+AAxvvZCkuZImpv0pkp6Q9LSkOZLGUgyyl6Va53+TNEzSnekeCySdlD57gKT7JC2U9HNAHT2EpP+Q9Hj6zIW7nbsmpc+RNCylvU3SPekzD0s6olt+mpY7foFRL5RqeGcC96Sk44CjIuKFFEC2RMR7JPUFfi/pPuBY4HCKawuOABYB03e77jDgBuDkdK0hEbFR0s+Av0TED1K+XwHXRMTvJB1McRbLkcCVwO8i4ipJHwJKmR3xN+ke/YEFku6MiA3APsBjEXGZpP+Vrn0xxRcLfT4inpd0AnA9cGonfoyWcw5+vUt/SU+l/YeBGyk2Rx+NiBdS+unAu1r784D9gfHAycCMiCgAL0t6cA/XnwTMa71WROxtXbsPABOkNyp2+0naN93jr9NnfytpUwnP9EVJH037Y1JZNwAtwK0p/d+BX6d7vBe4vc29+5ZwD7O3cPDrXV6PiGPaJqQgsLVtEnBJRNy7W76zurEcdcCkiNi+h7KUTNJkioH0xIjYJmku0G8v2SPdd/PuPwOzznCfX+25F/iCpAYASYdJ2geYB3wy9QmOBE7Zw2cfAU6WNC59dkhKfw0Y2CbffcAlrQeSjkm784BPpbQzgcEdlHV/YFMKfEdQrHm2qgNaa6+foticfhV4QdLH0z0k6egO7mG2Rw5+tefnFPvznkgv4fk/FGv4vwGeT+d+SXHlkjeJiHXAhRSbmE+zq9l5F/DR1gEP4IvAxDSgsohdo87/RDF4LqTY/H2pg7LeA/SRtBj4HsXg22orcHx6hlOBq1L6ucAFqXwL8asBrJO8qouZ5ZJrfmaWSw5+ZpZLDn5mlksOfmaWSw5+ZpZLDn5mlksOfmaWS/8fm9lk9MQtPUsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "data_list=[{\"Accuracy\":Accuracy,\"Precision\":Precision,\"Recall\":Recall,\"ROC_AUC\":ROC_AUC,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"tp\":tp,\"specificity\":specificity}]\n",
    "df=pd.DataFrame(data_list)\n",
    "\n",
    "filepath=os.path.join(base_dir,r\"acc_{}_{}\".format(all_teachers,shot))\n",
    "if not os.path.exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "df.to_excel(os.path.join(filepath,\"{}_{}.xlsx\".format(teacher_id,shot)),index=False)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7080bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
