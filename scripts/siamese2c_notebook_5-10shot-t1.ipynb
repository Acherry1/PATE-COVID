{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import utils as utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a16c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teachers=5\n",
    "teacher_id = 1\n",
    "epochs = 175\n",
    "shot = 10\n",
    "SIAMESE_MODEL_NAME = '..\\scripts\\models_h5\\siamese_network2c-t{}_{}notebook.h5'.format(teacher_id,shot)\n",
    "EMBEDDING_MODEL_NAME = 'embedding_network2w_0801.h5'\n",
    "if os.path.exists(SIAMESE_MODEL_NAME):\n",
    "    os.remove(SIAMESE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b86c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n",
      "The train set contains 20\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"F:\\chest project dataset\\dataset\"\n",
    "dataset_path = os.path.join( r\"F:\\chest project dataset\\auto_datasets\", r\"auto_t{}_{}\\t{}-{}\".format(all_teachers,shot,teacher_id, shot))\n",
    "result_file_path = os.path.join(base_dir, r\"true_result_xlsx\")\n",
    "pre_results_path = os.path.join(base_dir, r\"teacher_results_xlsx\",\"result_{}_{}\".format(all_teachers,shot))\n",
    "train_image_list, train_y_list = utils.load_images(dataset_path, 'train', (100, 100))\n",
    "print(\"The train set contains\", len(train_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8942f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The valid set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_path = r\"F:\\chest project dataset\\dataset\\pretrain_2c_0727\"\n",
    "valid_image_list, valid_y_list = utils.load_images(test_path, 'valid', (100, 100))\n",
    "print(\"The valid set contains\", len(valid_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ff552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The test set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_image_list, test_y_list = utils.load_images(test_path, 'test', (100, 100))\n",
    "print(\"The test set contains\", len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369794ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "num_classes: 2\n",
      "num_classes: 2\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train, source_labels_train, true_labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val, source_labels_val, true_labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_test, labels_test, source_labels_test, true_labels_test = utils.make_pairs(test_image_list, test_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8959873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c8f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for train 40\n",
      "number of pairs for validation 2400\n",
      "number of pairs for test 2400\n"
     ]
    }
   ],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x1(如何给标签带上)\n",
    "x_train_2 = pairs_train[:, 1]  # x2\n",
    "print(\"number of pairs for train\", np.shape(x_train_1)[0])\n",
    "\n",
    "x_val_1 = pairs_val[:, 0]\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0])\n",
    "\n",
    "x_test_1 = pairs_test[:, 0]\n",
    "x_test_2 = pairs_test[:, 1]\n",
    "# print(x_test_1)\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6955c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(result_file_path):\n",
    "    os.makedirs(result_file_path)\n",
    "result_xlsx=os.path.join(result_file_path,\"true_labels.xlsx\")\n",
    "if not os.path.exists(result_xlsx):\n",
    "    test_1_image_list = x_test_1.tolist()\n",
    "    df = pd.DataFrame({\"image\": test_1_image_list, \"true_label\": true_labels_test})\n",
    "    df.to_excel(result_xlsx, index=False)\n",
    "    df = pd.read_excel(result_xlsx)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_excel(result_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80363b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_1 = Input((100, 100, 3))\n",
    "input_2 = Input((100, 100, 3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_NAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for layer in embedding_network.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2'))\n",
    "\n",
    "output_1 = model(input_1)\n",
    "output_2 = model(input_2)\n",
    "\n",
    "merge_layer = Lambda(utils.manhattan_distance)([output_1, output_2])\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=SIAMESE_MODEL_NAME, verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b049d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.6750\n",
      "Epoch 00001: val_loss improved from inf to 0.33944, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 220s 6s/step - loss: 0.2167 - accuracy: 0.6750 - val_loss: 0.3394 - val_accuracy: 0.5117 - lr: 1.0000e-04\n",
      "Epoch 2/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2043 - accuracy: 0.7000\n",
      "Epoch 00002: val_loss improved from 0.33944 to 0.32178, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 220s 6s/step - loss: 0.2043 - accuracy: 0.7000 - val_loss: 0.3218 - val_accuracy: 0.5192 - lr: 1.0000e-04\n",
      "Epoch 3/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.7500\n",
      "Epoch 00003: val_loss improved from 0.32178 to 0.30395, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 246s 6s/step - loss: 0.1921 - accuracy: 0.7500 - val_loss: 0.3040 - val_accuracy: 0.5329 - lr: 1.0000e-04\n",
      "Epoch 4/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.8250\n",
      "Epoch 00004: val_loss improved from 0.30395 to 0.28385, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 244s 6s/step - loss: 0.1824 - accuracy: 0.8250 - val_loss: 0.2839 - val_accuracy: 0.5467 - lr: 1.0000e-04\n",
      "Epoch 5/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1700 - accuracy: 0.8250\n",
      "Epoch 00005: val_loss improved from 0.28385 to 0.26845, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 237s 6s/step - loss: 0.1700 - accuracy: 0.8250 - val_loss: 0.2684 - val_accuracy: 0.5663 - lr: 1.0000e-04\n",
      "Epoch 6/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1611 - accuracy: 0.8250\n",
      "Epoch 00006: val_loss improved from 0.26845 to 0.24870, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 218s 6s/step - loss: 0.1611 - accuracy: 0.8250 - val_loss: 0.2487 - val_accuracy: 0.5825 - lr: 1.0000e-04\n",
      "Epoch 7/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.8250\n",
      "Epoch 00007: val_loss improved from 0.24870 to 0.22937, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 212s 5s/step - loss: 0.1510 - accuracy: 0.8250 - val_loss: 0.2294 - val_accuracy: 0.6033 - lr: 1.0000e-04\n",
      "Epoch 8/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1438 - accuracy: 0.8500\n",
      "Epoch 00008: val_loss improved from 0.22937 to 0.21162, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 213s 5s/step - loss: 0.1438 - accuracy: 0.8500 - val_loss: 0.2116 - val_accuracy: 0.6342 - lr: 1.0000e-04\n",
      "Epoch 9/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.8750\n",
      "Epoch 00009: val_loss did not improve from 0.21162\n",
      "40/40 [==============================] - 229s 6s/step - loss: 0.1400 - accuracy: 0.8750 - val_loss: 0.2122 - val_accuracy: 0.6408 - lr: 1.0000e-04\n",
      "Epoch 10/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.8750\n",
      "Epoch 00010: val_loss improved from 0.21162 to 0.20390, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 234s 6s/step - loss: 0.1399 - accuracy: 0.8750 - val_loss: 0.2039 - val_accuracy: 0.6604 - lr: 1.0000e-04\n",
      "Epoch 11/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1387 - accuracy: 0.8750\n",
      "Epoch 00011: val_loss improved from 0.20390 to 0.20148, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 220s 6s/step - loss: 0.1387 - accuracy: 0.8750 - val_loss: 0.2015 - val_accuracy: 0.6692 - lr: 1.0000e-04\n",
      "Epoch 12/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9000\n",
      "Epoch 00012: val_loss did not improve from 0.20148\n",
      "40/40 [==============================] - 212s 5s/step - loss: 0.1383 - accuracy: 0.9000 - val_loss: 0.2068 - val_accuracy: 0.6650 - lr: 1.0000e-04\n",
      "Epoch 13/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1380 - accuracy: 0.8750\n",
      "Epoch 00013: val_loss did not improve from 0.20148\n",
      "40/40 [==============================] - 220s 6s/step - loss: 0.1380 - accuracy: 0.8750 - val_loss: 0.2120 - val_accuracy: 0.6604 - lr: 1.0000e-04\n",
      "Epoch 14/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1393 - accuracy: 0.8750\n",
      "Epoch 00014: val_loss did not improve from 0.20148\n",
      "40/40 [==============================] - 225s 6s/step - loss: 0.1393 - accuracy: 0.8750 - val_loss: 0.2030 - val_accuracy: 0.6762 - lr: 1.0000e-04\n",
      "Epoch 15/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1357 - accuracy: 0.9250\n",
      "Epoch 00015: val_loss did not improve from 0.20148\n",
      "40/40 [==============================] - 215s 6s/step - loss: 0.1357 - accuracy: 0.9250 - val_loss: 0.2068 - val_accuracy: 0.6746 - lr: 1.0000e-04\n",
      "Epoch 16/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.8750\n",
      "Epoch 00016: val_loss did not improve from 0.20148\n",
      "40/40 [==============================] - 227s 6s/step - loss: 0.1352 - accuracy: 0.8750 - val_loss: 0.2032 - val_accuracy: 0.6829 - lr: 1.0000e-04\n",
      "Epoch 17/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9250\n",
      "Epoch 00017: val_loss improved from 0.20148 to 0.19889, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 222s 6s/step - loss: 0.1346 - accuracy: 0.9250 - val_loss: 0.1989 - val_accuracy: 0.6896 - lr: 2.0000e-05\n",
      "Epoch 18/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1346 - accuracy: 0.9250\n",
      "Epoch 00018: val_loss did not improve from 0.19889\n",
      "40/40 [==============================] - 232s 6s/step - loss: 0.1346 - accuracy: 0.9250 - val_loss: 0.2026 - val_accuracy: 0.6842 - lr: 2.0000e-05\n",
      "Epoch 19/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9250"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "                      labels_train,\n",
    "                      validation_data=([x_val_1, x_val_2], labels_val),\n",
    "                      batch_size=1,\n",
    "                      epochs=epochs,  # 175 for contrastive 100 for cross ent\n",
    "                      callbacks=[checkpointer, early_stopping, reduce_lr]\n",
    "                      )\n",
    "# print()\n",
    "# Plot the accuracy\n",
    "# utils.plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "# utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020c970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "# 返回的是TRUE或FALSE,没有标签数据怎么知道他被分到哪儿个类中？\n",
    "y_pred = Y_pred > 0.5\n",
    "# x1,和x2是否匹配：匹配1，不匹配0\n",
    "y_test = labels_test\n",
    "print(\"\\nEvaluate on validation data\")\n",
    "Accuracy=accuracy_score(y_test, y_pred)\n",
    "Precision=precision_score(y_test, y_pred, average='weighted')\n",
    "Recall=recall_score(y_test, y_pred, average='weighted')\n",
    "ROC_AUC=roc_auc_score(y_test, y_pred, average='weighted')\n",
    "F1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "print(\"Precision:\", Precision)\n",
    "print(\"Recall:\", Recall)\n",
    "print(\"ROC AUC:\", ROC_AUC)\n",
    "print(\"F1:\",F1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if i else 0 for i in y_pred]\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] == 1:\n",
    "        pred_labels += [source_labels_test[i]]\n",
    "    else:\n",
    "        if source_labels_test[i] == 1:\n",
    "            pred_labels += [0.0]\n",
    "        else:\n",
    "            pred_labels += [1.0]\n",
    "a = x_test_1.tolist()\n",
    "df = pd.DataFrame({\"image\": a, \"label_{}\".format(teacher_id): pred_labels})\n",
    "if not os.path.exists(pre_results_path):\n",
    "    os.makedirs(pre_results_path)\n",
    "df.to_excel(os.path.join(pre_results_path,\"{}_{}_{}.xlsx\".format(epochs,teacher_id,shot)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "data_list=[{\"Accuracy\":Accuracy,\"Precision\":Precision,\"Recall\":Recall,\"ROC_AUC\":ROC_AUC,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"tp\":tp,\"specificity\":specificity}]\n",
    "df=pd.DataFrame(data_list)\n",
    "\n",
    "filepath=os.path.join(base_dir,r\"acc_{}_{}\".format(all_teachers,shot))\n",
    "if not os.path.exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "df.to_excel(os.path.join(filepath,\"{}_{}.xlsx\".format(teacher_id,shot)),index=False)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7080bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
