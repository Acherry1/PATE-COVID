{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import utils as utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a16c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teachers=3\n",
    "teacher_id = 1\n",
    "epochs = 175\n",
    "shot = 10\n",
    "SIAMESE_MODEL_NAME = '..\\scripts\\models_h5\\siamese_network2c-t{}_{}notebook.h5'.format(teacher_id,shot)\n",
    "EMBEDDING_MODEL_NAME = 'embedding_network2w_0801.h5'\n",
    "if os.path.exists(SIAMESE_MODEL_NAME):\n",
    "    os.remove(SIAMESE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b86c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n",
      "The train set contains 20\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"..\\scripts\\dataset\"\n",
    "dataset_path = os.path.join( r\"..\\scripts\\auto_datasets\", r\"auto_t{}_{}\\t{}-{}\".format(all_teachers,shot,teacher_id, shot))\n",
    "result_file_path = os.path.join(base_dir, r\"true_result_xlsx\")\n",
    "pre_results_path = os.path.join(base_dir, r\"teacher_results_xlsx\",\"result_{}_{}\".format(all_teachers,shot))\n",
    "train_image_list, train_y_list = utils.load_images(dataset_path, 'train', (100, 100))\n",
    "print(\"The train set contains\", len(train_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8942f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The valid set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_path = r\"../scripts/dataset/pretrain_2c_0727/\"\n",
    "valid_image_list, valid_y_list = utils.load_images(test_path, 'valid', (100, 100))\n",
    "print(\"The valid set contains\", len(valid_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ff552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The test set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_image_list, test_y_list = utils.load_images(test_path, 'test', (100, 100))\n",
    "print(\"The test set contains\", len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369794ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "num_classes: 2\n",
      "num_classes: 2\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train, source_labels_train, true_labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val, source_labels_val, true_labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_test, labels_test, source_labels_test, true_labels_test = utils.make_pairs(test_image_list, test_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8959873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c8f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for train 40\n",
      "number of pairs for validation 2400\n",
      "number of pairs for test 2400\n"
     ]
    }
   ],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x1(如何给标签带上)\n",
    "x_train_2 = pairs_train[:, 1]  # x2\n",
    "print(\"number of pairs for train\", np.shape(x_train_1)[0])\n",
    "\n",
    "x_val_1 = pairs_val[:, 0]\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0])\n",
    "\n",
    "x_test_1 = pairs_test[:, 0]\n",
    "x_test_2 = pairs_test[:, 1]\n",
    "# print(x_test_1)\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6955c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(result_file_path):\n",
    "    os.makedirs(result_file_path)\n",
    "result_xlsx=os.path.join(result_file_path,\"true_labels.xlsx\")\n",
    "if not os.path.exists(result_xlsx):\n",
    "    test_1_image_list = x_test_1.tolist()\n",
    "    df = pd.DataFrame({\"image\": test_1_image_list, \"true_label\": true_labels_test})\n",
    "    df.to_excel(result_xlsx, index=False)\n",
    "    df = pd.read_excel(result_xlsx)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_excel(result_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80363b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_1 = Input((100, 100, 3))\n",
    "input_2 = Input((100, 100, 3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_NAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for layer in embedding_network.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2'))\n",
    "\n",
    "output_1 = model(input_1)\n",
    "output_2 = model(input_2)\n",
    "\n",
    "merge_layer = Lambda(utils.manhattan_distance)([output_1, output_2])\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=SIAMESE_MODEL_NAME, verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b049d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1753 - accuracy: 0.6000\n",
      "Epoch 00001: val_loss improved from inf to 0.32894, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 413s 10s/step - loss: 0.1753 - accuracy: 0.6000 - val_loss: 0.3289 - val_accuracy: 0.5100 - lr: 1.0000e-04\n",
      "Epoch 2/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.6500\n",
      "Epoch 00002: val_loss improved from 0.32894 to 0.31167, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 406s 10s/step - loss: 0.1634 - accuracy: 0.6500 - val_loss: 0.3117 - val_accuracy: 0.5238 - lr: 1.0000e-04\n",
      "Epoch 3/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.6750\n",
      "Epoch 00003: val_loss improved from 0.31167 to 0.29384, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 413s 11s/step - loss: 0.1546 - accuracy: 0.6750 - val_loss: 0.2938 - val_accuracy: 0.5379 - lr: 1.0000e-04\n",
      "Epoch 4/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.7000\n",
      "Epoch 00004: val_loss improved from 0.29384 to 0.27890, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 410s 10s/step - loss: 0.1479 - accuracy: 0.7000 - val_loss: 0.2789 - val_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 5/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.7250\n",
      "Epoch 00005: val_loss improved from 0.27890 to 0.26672, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 413s 11s/step - loss: 0.1428 - accuracy: 0.7250 - val_loss: 0.2667 - val_accuracy: 0.5671 - lr: 1.0000e-04\n",
      "Epoch 6/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.7250\n",
      "Epoch 00006: val_loss improved from 0.26672 to 0.25528, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 413s 11s/step - loss: 0.1394 - accuracy: 0.7250 - val_loss: 0.2553 - val_accuracy: 0.5863 - lr: 1.0000e-04\n",
      "Epoch 7/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.8000\n",
      "Epoch 00007: val_loss improved from 0.25528 to 0.24423, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 411s 11s/step - loss: 0.1365 - accuracy: 0.8000 - val_loss: 0.2442 - val_accuracy: 0.5987 - lr: 1.0000e-04\n",
      "Epoch 8/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1345 - accuracy: 0.8250\n",
      "Epoch 00008: val_loss improved from 0.24423 to 0.23293, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 415s 11s/step - loss: 0.1345 - accuracy: 0.8250 - val_loss: 0.2329 - val_accuracy: 0.6167 - lr: 1.0000e-04\n",
      "Epoch 9/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.8500\n",
      "Epoch 00009: val_loss improved from 0.23293 to 0.22854, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 410s 11s/step - loss: 0.1330 - accuracy: 0.8500 - val_loss: 0.2285 - val_accuracy: 0.6250 - lr: 1.0000e-04\n",
      "Epoch 10/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9000 ETA: \n",
      "Epoch 00010: val_loss improved from 0.22854 to 0.22133, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 400s 10s/step - loss: 0.1319 - accuracy: 0.9000 - val_loss: 0.2213 - val_accuracy: 0.6400 - lr: 1.0000e-04\n",
      "Epoch 11/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9250\n",
      "Epoch 00011: val_loss improved from 0.22133 to 0.21785, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 398s 10s/step - loss: 0.1306 - accuracy: 0.9250 - val_loss: 0.2178 - val_accuracy: 0.6479 - lr: 1.0000e-04\n",
      "Epoch 12/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9250\n",
      "Epoch 00012: val_loss improved from 0.21785 to 0.21276, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 396s 10s/step - loss: 0.1299 - accuracy: 0.9250 - val_loss: 0.2128 - val_accuracy: 0.6583 - lr: 1.0000e-04\n",
      "Epoch 13/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1295 - accuracy: 0.9250\n",
      "Epoch 00013: val_loss improved from 0.21276 to 0.21191, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 398s 10s/step - loss: 0.1295 - accuracy: 0.9250 - val_loss: 0.2119 - val_accuracy: 0.6642 - lr: 1.0000e-04\n",
      "Epoch 14/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9250\n",
      "Epoch 00014: val_loss improved from 0.21191 to 0.21081, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 397s 10s/step - loss: 0.1287 - accuracy: 0.9250 - val_loss: 0.2108 - val_accuracy: 0.6658 - lr: 1.0000e-04\n",
      "Epoch 15/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1285 - accuracy: 0.9250\n",
      "Epoch 00015: val_loss improved from 0.21081 to 0.21060, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 404s 10s/step - loss: 0.1285 - accuracy: 0.9250 - val_loss: 0.2106 - val_accuracy: 0.6692 - lr: 1.0000e-04\n",
      "Epoch 16/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9500\n",
      "Epoch 00016: val_loss did not improve from 0.21060\n",
      "40/40 [==============================] - 397s 10s/step - loss: 0.1279 - accuracy: 0.9500 - val_loss: 0.2179 - val_accuracy: 0.6642 - lr: 1.0000e-04\n",
      "Epoch 17/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9500 ETA: 2s - los\n",
      "Epoch 00017: val_loss did not improve from 0.21060\n",
      "40/40 [==============================] - 450s 12s/step - loss: 0.1273 - accuracy: 0.9500 - val_loss: 0.2123 - val_accuracy: 0.6725 - lr: 1.0000e-04\n",
      "Epoch 18/175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9500\n",
      "Epoch 00018: val_loss did not improve from 0.21060\n",
      "40/40 [==============================] - 442s 11s/step - loss: 0.1268 - accuracy: 0.9500 - val_loss: 0.2135 - val_accuracy: 0.6729 - lr: 1.0000e-04\n",
      "Epoch 19/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9750\n",
      "Epoch 00019: val_loss improved from 0.21060 to 0.20928, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 443s 11s/step - loss: 0.1264 - accuracy: 0.9750 - val_loss: 0.2093 - val_accuracy: 0.6783 - lr: 1.0000e-04\n",
      "Epoch 20/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 0.9750\n",
      "Epoch 00020: val_loss did not improve from 0.20928\n",
      "40/40 [==============================] - 480s 12s/step - loss: 0.1262 - accuracy: 0.9750 - val_loss: 0.2135 - val_accuracy: 0.6762 - lr: 1.0000e-04\n",
      "Epoch 21/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1256 - accuracy: 0.9750\n",
      "Epoch 00021: val_loss did not improve from 0.20928\n",
      "40/40 [==============================] - 437s 11s/step - loss: 0.1256 - accuracy: 0.9750 - val_loss: 0.2099 - val_accuracy: 0.6821 - lr: 1.0000e-04\n",
      "Epoch 22/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9750\n",
      "Epoch 00022: val_loss improved from 0.20928 to 0.20393, saving model to ..\\scripts\\models_h5\\siamese_network2c-t1_10notebook.h5\n",
      "40/40 [==============================] - 414s 11s/step - loss: 0.1253 - accuracy: 0.9750 - val_loss: 0.2039 - val_accuracy: 0.6946 - lr: 1.0000e-04\n",
      "Epoch 23/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1248 - accuracy: 0.9750 ETA: 1s - loss: 0.1353 - \n",
      "Epoch 00023: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 435s 11s/step - loss: 0.1248 - accuracy: 0.9750 - val_loss: 0.2088 - val_accuracy: 0.6925 - lr: 1.0000e-04\n",
      "Epoch 24/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9750\n",
      "Epoch 00024: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 444s 11s/step - loss: 0.1247 - accuracy: 0.9750 - val_loss: 0.2104 - val_accuracy: 0.6925 - lr: 1.0000e-04\n",
      "Epoch 25/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1245 - accuracy: 0.9750\n",
      "Epoch 00025: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 480s 12s/step - loss: 0.1245 - accuracy: 0.9750 - val_loss: 0.2087 - val_accuracy: 0.6946 - lr: 1.0000e-04\n",
      "Epoch 26/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9750\n",
      "Epoch 00026: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 479s 12s/step - loss: 0.1241 - accuracy: 0.9750 - val_loss: 0.2083 - val_accuracy: 0.6971 - lr: 1.0000e-04\n",
      "Epoch 27/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9750\n",
      "Epoch 00027: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 460s 12s/step - loss: 0.1236 - accuracy: 0.9750 - val_loss: 0.2135 - val_accuracy: 0.6938 - lr: 1.0000e-04\n",
      "Epoch 28/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9750\n",
      "Epoch 00028: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 465s 12s/step - loss: 0.1233 - accuracy: 0.9750 - val_loss: 0.2122 - val_accuracy: 0.6942 - lr: 2.0000e-05\n",
      "Epoch 29/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1232 - accuracy: 0.9750\n",
      "Epoch 00029: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 460s 12s/step - loss: 0.1232 - accuracy: 0.9750 - val_loss: 0.2123 - val_accuracy: 0.6946 - lr: 2.0000e-05\n",
      "Epoch 30/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9750\n",
      "Epoch 00030: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 458s 12s/step - loss: 0.1231 - accuracy: 0.9750 - val_loss: 0.2107 - val_accuracy: 0.6963 - lr: 2.0000e-05\n",
      "Epoch 31/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9750\n",
      "Epoch 00031: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 468s 12s/step - loss: 0.1230 - accuracy: 0.9750 - val_loss: 0.2096 - val_accuracy: 0.6996 - lr: 2.0000e-05\n",
      "Epoch 32/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1230 - accuracy: 0.9750\n",
      "Epoch 00032: val_loss did not improve from 0.20393\n",
      "40/40 [==============================] - 461s 12s/step - loss: 0.1230 - accuracy: 0.9750 - val_loss: 0.2096 - val_accuracy: 0.6996 - lr: 2.0000e-05\n",
      "Epoch 00032: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "                      labels_train,\n",
    "                      validation_data=([x_val_1, x_val_2], labels_val),\n",
    "                      batch_size=1,\n",
    "                      epochs=epochs,  # 175 for contrastive 100 for cross ent\n",
    "                      callbacks=[checkpointer, early_stopping, reduce_lr]\n",
    "                      )\n",
    "# print()\n",
    "# Plot the accuracy\n",
    "# utils.plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "# utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a897dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 312s 4s/step - loss: 0.1482 - accuracy: 0.8712\n",
      "test loss, test acc: [0.14818012714385986, 0.8712499737739563]\n"
     ]
    }
   ],
   "source": [
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5020c970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on validation data\n",
      "Accuracy: 0.87125\n",
      "Precision: 0.8976143141153081\n",
      "Recall: 0.87125\n",
      "ROC AUC: 0.8712500000000001\n",
      "F1: 0.8690797930062553\n"
     ]
    }
   ],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "# 返回的是TRUE或FALSE,没有标签数据怎么知道他被分到哪儿个类中？\n",
    "y_pred = Y_pred > 0.5\n",
    "# x1,和x2是否匹配：匹配1，不匹配0\n",
    "y_test = labels_test\n",
    "print(\"\\nEvaluate on validation data\")\n",
    "Accuracy=accuracy_score(y_test, y_pred)\n",
    "Precision=precision_score(y_test, y_pred, average='weighted')\n",
    "Recall=recall_score(y_test, y_pred, average='weighted')\n",
    "ROC_AUC=roc_auc_score(y_test, y_pred, average='weighted')\n",
    "F1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "print(\"Precision:\", Precision)\n",
    "print(\"Recall:\", Recall)\n",
    "print(\"ROC AUC:\", ROC_AUC)\n",
    "print(\"F1:\",F1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if i else 0 for i in y_pred]\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] == 1:\n",
    "        pred_labels += [source_labels_test[i]]\n",
    "    else:\n",
    "        if source_labels_test[i] == 1:\n",
    "            pred_labels += [0.0]\n",
    "        else:\n",
    "            pred_labels += [1.0]\n",
    "a = x_test_1.tolist()\n",
    "df = pd.DataFrame({\"image\": a, \"label_{}\".format(teacher_id): pred_labels})\n",
    "if not os.path.exists(pre_results_path):\n",
    "    os.makedirs(pre_results_path)\n",
    "df.to_excel(os.path.join(pre_results_path,\"{}_{}_{}.xlsx\".format(epochs,teacher_id,shot)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ad007f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "data_list=[{\"Accuracy\":Accuracy,\"Precision\":Precision,\"Recall\":Recall,\"ROC_AUC\":ROC_AUC,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"tp\":tp,\"specificity\":specificity}]\n",
    "df=pd.DataFrame(data_list)\n",
    "\n",
    "filepath=os.path.join(base_dir,r\"acc_{}_{}\".format(all_teachers,shot))\n",
    "if not os.path.exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "df.to_excel(os.path.join(filepath,\"{}_{}.xlsx\".format(teacher_id,shot)),index=False)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7080bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
