{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65c72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import utils as utils\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a16c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_teachers=2\n",
    "teacher_id = 2\n",
    "epochs = 175\n",
    "shot = 20\n",
    "SIAMESE_MODEL_NAME = '..\\scripts\\models_h5\\siamese_network2c-t{}_{}notebook.h5'.format(teacher_id,shot)\n",
    "EMBEDDING_MODEL_NAME = 'embedding_network2w_0801.h5'\n",
    "if os.path.exists(SIAMESE_MODEL_NAME):\n",
    "    os.remove(SIAMESE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b86c631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n",
      "The train set contains 20\n"
     ]
    }
   ],
   "source": [
    "base_dir = r\"..\\scripts\\dataset\"\n",
    "dataset_path = os.path.join( r\"..\\scripts\\auto_datasets\", r\"auto_t{}_{}\\t{}-{}\".format(all_teachers,shot,teacher_id, shot))\n",
    "result_file_path = os.path.join(base_dir, r\"true_result_xlsx\")\n",
    "pre_results_path = os.path.join(base_dir, r\"teacher_results_xlsx\",\"result_{}_{}\".format(all_teachers,shot))\n",
    "train_image_list, train_y_list = utils.load_images(dataset_path, 'train', (100, 100))\n",
    "print(\"The train set contains\", len(train_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8942f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The valid set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_path = r\"../scripts/dataset/pretrain_2c_0727/\"\n",
    "valid_image_list, valid_y_list = utils.load_images(test_path, 'valid', (100, 100))\n",
    "print(\"The valid set contains\", len(valid_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ff552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n",
      "The test set contains 1200\n"
     ]
    }
   ],
   "source": [
    "test_image_list, test_y_list = utils.load_images(test_path, 'test', (100, 100))\n",
    "print(\"The test set contains\", len(test_image_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369794ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "num_classes: 2\n",
      "num_classes: 2\n"
     ]
    }
   ],
   "source": [
    "# make train pairs\n",
    "pairs_train, labels_train, source_labels_train, true_labels_train = utils.make_pairs(train_image_list, train_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_val, labels_val, source_labels_val, true_labels_val = utils.make_pairs(valid_image_list, valid_y_list)\n",
    "\n",
    "# make validation pairs\n",
    "pairs_test, labels_test, source_labels_test, true_labels_test = utils.make_pairs(test_image_list, test_y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8959873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c8f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pairs for train 40\n",
      "number of pairs for validation 2400\n",
      "number of pairs for test 2400\n"
     ]
    }
   ],
   "source": [
    "x_train_1 = pairs_train[:, 0]  # x1(如何给标签带上)\n",
    "x_train_2 = pairs_train[:, 1]  # x2\n",
    "print(\"number of pairs for train\", np.shape(x_train_1)[0])\n",
    "\n",
    "x_val_1 = pairs_val[:, 0]\n",
    "x_val_2 = pairs_val[:, 1]\n",
    "print(\"number of pairs for validation\", np.shape(x_val_1)[0])\n",
    "\n",
    "x_test_1 = pairs_test[:, 0]\n",
    "x_test_2 = pairs_test[:, 1]\n",
    "# print(x_test_1)\n",
    "print(\"number of pairs for test\", np.shape(x_test_1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6955c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(result_file_path):\n",
    "    os.makedirs(result_file_path)\n",
    "result_xlsx=os.path.join(result_file_path,\"true_labels.xlsx\")\n",
    "if not os.path.exists(result_xlsx):\n",
    "    test_1_image_list = x_test_1.tolist()\n",
    "    df = pd.DataFrame({\"image\": test_1_image_list, \"true_label\": true_labels_test})\n",
    "    df.to_excel(result_xlsx, index=False)\n",
    "    df = pd.read_excel(result_xlsx)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.to_excel(result_xlsx, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80363b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "input_1 = Input((100, 100, 3))\n",
    "input_2 = Input((100, 100, 3))\n",
    "\n",
    "embedding_network = tf.keras.models.load_model(EMBEDDING_MODEL_NAME)\n",
    "embedding_network.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "for layer in embedding_network.layers:\n",
    "    model.add(layer)\n",
    "\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(5120, name='den', activation='sigmoid', kernel_regularizer='l2'))\n",
    "\n",
    "output_1 = model(input_1)\n",
    "output_2 = model(input_2)\n",
    "\n",
    "merge_layer = Lambda(utils.manhattan_distance)([output_1, output_2])\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(merge_layer)\n",
    "siamese = Model(inputs=[input_1, input_2], outputs=output_layer)\n",
    "siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c9f7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.0001)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=SIAMESE_MODEL_NAME, verbose=1,\n",
    "                               save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b049d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 5120)         14739266    ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 1)            0           ['sequential[0][0]',             \n",
      "                                                                  'sequential[1][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            2           ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,739,268\n",
      "Trainable params: 15,362\n",
      "Non-trainable params: 14,723,906\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.6500\n",
      "Epoch 00001: val_loss improved from inf to 0.34377, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 233s 6s/step - loss: 0.2791 - accuracy: 0.6500 - val_loss: 0.3438 - val_accuracy: 0.5046 - lr: 1.0000e-04\n",
      "Epoch 2/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2604 - accuracy: 0.7250\n",
      "Epoch 00002: val_loss improved from 0.34377 to 0.32294, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 214s 5s/step - loss: 0.2604 - accuracy: 0.7250 - val_loss: 0.3229 - val_accuracy: 0.5171 - lr: 1.0000e-04\n",
      "Epoch 3/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2412 - accuracy: 0.7250\n",
      "Epoch 00003: val_loss improved from 0.32294 to 0.30130, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 208s 5s/step - loss: 0.2412 - accuracy: 0.7250 - val_loss: 0.3013 - val_accuracy: 0.5321 - lr: 1.0000e-04\n",
      "Epoch 4/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.7750\n",
      "Epoch 00004: val_loss improved from 0.30130 to 0.27650, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 205s 5s/step - loss: 0.2183 - accuracy: 0.7750 - val_loss: 0.2765 - val_accuracy: 0.5542 - lr: 1.0000e-04\n",
      "Epoch 5/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.7750\n",
      "Epoch 00005: val_loss improved from 0.27650 to 0.25310, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 207s 5s/step - loss: 0.1935 - accuracy: 0.7750 - val_loss: 0.2531 - val_accuracy: 0.5792 - lr: 1.0000e-04\n",
      "Epoch 6/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.8000\n",
      "Epoch 00006: val_loss improved from 0.25310 to 0.22719, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 203s 5s/step - loss: 0.1741 - accuracy: 0.8000 - val_loss: 0.2272 - val_accuracy: 0.6071 - lr: 1.0000e-04\n",
      "Epoch 7/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.8250\n",
      "Epoch 00007: val_loss improved from 0.22719 to 0.21623, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 204s 5s/step - loss: 0.1574 - accuracy: 0.8250 - val_loss: 0.2162 - val_accuracy: 0.6221 - lr: 1.0000e-04\n",
      "Epoch 8/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.8250\n",
      "Epoch 00008: val_loss improved from 0.21623 to 0.20308, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 202s 5s/step - loss: 0.1537 - accuracy: 0.8250 - val_loss: 0.2031 - val_accuracy: 0.6508 - lr: 1.0000e-04\n",
      "Epoch 9/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1539 - accuracy: 0.8250\n",
      "Epoch 00009: val_loss did not improve from 0.20308\n",
      "40/40 [==============================] - 201s 5s/step - loss: 0.1539 - accuracy: 0.8250 - val_loss: 0.2050 - val_accuracy: 0.6533 - lr: 1.0000e-04\n",
      "Epoch 10/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.8250\n",
      "Epoch 00010: val_loss did not improve from 0.20308\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1502 - accuracy: 0.8250 - val_loss: 0.2071 - val_accuracy: 0.6554 - lr: 1.0000e-04\n",
      "Epoch 11/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1513 - accuracy: 0.8250\n",
      "Epoch 00011: val_loss did not improve from 0.20308\n",
      "40/40 [==============================] - 200s 5s/step - loss: 0.1513 - accuracy: 0.8250 - val_loss: 0.2045 - val_accuracy: 0.6642 - lr: 1.0000e-04\n",
      "Epoch 12/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1504 - accuracy: 0.8250\n",
      "Epoch 00012: val_loss improved from 0.20308 to 0.19431, saving model to ..\\scripts\\models_h5\\siamese_network2c-t2_10notebook.h5\n",
      "40/40 [==============================] - 200s 5s/step - loss: 0.1504 - accuracy: 0.8250 - val_loss: 0.1943 - val_accuracy: 0.6875 - lr: 1.0000e-04\n",
      "Epoch 13/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.8250\n",
      "Epoch 00013: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1485 - accuracy: 0.8250 - val_loss: 0.1981 - val_accuracy: 0.6787 - lr: 1.0000e-04\n",
      "Epoch 14/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.8250\n",
      "Epoch 00014: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 200s 5s/step - loss: 0.1489 - accuracy: 0.8250 - val_loss: 0.2045 - val_accuracy: 0.6721 - lr: 1.0000e-04\n",
      "Epoch 15/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1494 - accuracy: 0.8250\n",
      "Epoch 00015: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1494 - accuracy: 0.8250 - val_loss: 0.2078 - val_accuracy: 0.6717 - lr: 1.0000e-04\n",
      "Epoch 16/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.8250 ETA\n",
      "Epoch 00016: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1512 - accuracy: 0.8250 - val_loss: 0.2144 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 17/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.8250\n",
      "Epoch 00017: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1497 - accuracy: 0.8250 - val_loss: 0.2046 - val_accuracy: 0.6804 - lr: 1.0000e-04\n",
      "Epoch 18/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.8250\n",
      "Epoch 00018: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1469 - accuracy: 0.8250 - val_loss: 0.1993 - val_accuracy: 0.6921 - lr: 2.0000e-05\n",
      "Epoch 19/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.8250\n",
      "Epoch 00019: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1461 - accuracy: 0.8250 - val_loss: 0.1951 - val_accuracy: 0.7004 - lr: 2.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.8250\n",
      "Epoch 00020: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1459 - accuracy: 0.8250 - val_loss: 0.1951 - val_accuracy: 0.7017 - lr: 2.0000e-05\n",
      "Epoch 21/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.8250\n",
      "Epoch 00021: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 199s 5s/step - loss: 0.1462 - accuracy: 0.8250 - val_loss: 0.1974 - val_accuracy: 0.6975 - lr: 2.0000e-05\n",
      "Epoch 22/175\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.1454 - accuracy: 0.8250\n",
      "Epoch 00022: val_loss did not improve from 0.19431\n",
      "40/40 [==============================] - 198s 5s/step - loss: 0.1454 - accuracy: 0.8250 - val_loss: 0.1963 - val_accuracy: 0.6992 - lr: 2.0000e-05\n",
      "Epoch 00022: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "siamese.compile(loss=utils.loss(1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "# siamese.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "siamese.summary()\n",
    "history = siamese.fit([x_train_1, x_train_2],\n",
    "                      labels_train,\n",
    "                      validation_data=([x_val_1, x_val_2], labels_val),\n",
    "                      batch_size=1,\n",
    "                      epochs=epochs,  # 175 for contrastive 100 for cross ent\n",
    "                      callbacks=[checkpointer, early_stopping, reduce_lr]\n",
    "                      )\n",
    "# print()\n",
    "# Plot the accuracy\n",
    "# utils.plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
    "\n",
    "# Plot the constrastive loss\n",
    "# utils.plt_metric(history=history.history, metric=\"loss\", title=\"Constrastive Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a897dfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 159s 2s/step - loss: 0.1512 - accuracy: 0.8808\n",
      "test loss, test acc: [0.15119001269340515, 0.8808333277702332]\n"
     ]
    }
   ],
   "source": [
    "results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5020c970",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate on validation data\n",
      "Accuracy: 0.8808333333333334\n",
      "Precision: 0.9037685060565275\n",
      "Recall: 0.8808333333333334\n",
      "ROC AUC: 0.8808333333333334\n",
      "F1: 0.8791167066206146\n"
     ]
    }
   ],
   "source": [
    "Y_pred = siamese.predict([x_test_1, x_test_2]).squeeze()\n",
    "# 返回的是TRUE或FALSE,没有标签数据怎么知道他被分到哪儿个类中？\n",
    "y_pred = Y_pred > 0.5\n",
    "# x1,和x2是否匹配：匹配1，不匹配0\n",
    "y_test = labels_test\n",
    "print(\"\\nEvaluate on validation data\")\n",
    "Accuracy=accuracy_score(y_test, y_pred)\n",
    "Precision=precision_score(y_test, y_pred, average='weighted')\n",
    "Recall=recall_score(y_test, y_pred, average='weighted')\n",
    "ROC_AUC=roc_auc_score(y_test, y_pred, average='weighted')\n",
    "F1=f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Accuracy:\", Accuracy)\n",
    "print(\"Precision:\", Precision)\n",
    "print(\"Recall:\", Recall)\n",
    "print(\"ROC AUC:\", ROC_AUC)\n",
    "print(\"F1:\",F1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bd41fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1 if i else 0 for i in y_pred]\n",
    "pred_labels = []\n",
    "\n",
    "for i in range(0, len(y_pred)):\n",
    "    if y_pred[i] == 1:\n",
    "        pred_labels += [source_labels_test[i]]\n",
    "    else:\n",
    "        if source_labels_test[i] == 1:\n",
    "            pred_labels += [0.0]\n",
    "        else:\n",
    "            pred_labels += [1.0]\n",
    "a = x_test_1.tolist()\n",
    "df = pd.DataFrame({\"image\": a, \"label_{}\".format(teacher_id): pred_labels})\n",
    "if not os.path.exists(pre_results_path):\n",
    "    os.makedirs(pre_results_path)\n",
    "df.to_excel(os.path.join(pre_results_path,\"{}_{}_{}.xlsx\".format(epochs,teacher_id,shot)),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ad007f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "print(\"Specificity:\", specificity)\n",
    "data_list=[{\"Accuracy\":Accuracy,\"Precision\":Precision,\"Recall\":Recall,\"ROC_AUC\":ROC_AUC,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"tp\":tp,\"specificity\":specificity}]\n",
    "df=pd.DataFrame(data_list)\n",
    "\n",
    "filepath=os.path.join(base_dir,r\"acc_{}_{}\".format(all_teachers,shot))\n",
    "if not os.path.exists(filepath):\n",
    "    os.mkdir(filepath)\n",
    "df.to_excel(os.path.join(filepath,\"{}_{}.xlsx\".format(teacher_id,shot)),index=False)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7080bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
